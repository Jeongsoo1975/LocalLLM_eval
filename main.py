import ollama
import pandas as pd
from openai import OpenAI, AuthenticationError, APIStatusError
import os
from dotenv import load_dotenv
import re
import time
import csv
import logging  # Î°úÍπÖ Î™®Îìà Ï∂îÍ∞Ä
import argparse  # Ïù∏Ïûê ÌååÏã± Î™®Îìà Ï∂îÍ∞Ä

# --- Î°úÍπÖ ÏÑ§Ï†ï ---
# Î°úÍ∑∏ ÌååÏùºÎ™Ö, Î°úÍ∑∏ Î†àÎ≤®, Î°úÍ∑∏ Ìè¨Îß∑ Îì±ÏùÑ ÏÑ§Ï†ïÌï©ÎãàÎã§.
# Î°úÍ∑∏ ÌååÏùºÏùÄ 'evaluation_log.txt'Ïóê Ï†ÄÏû•ÎêòÎ©∞, Ïã§Ìñâ ÏãúÎßàÎã§ ÎÇ¥Ïö©Ïù¥ Ï∂îÍ∞ÄÎê©ÎãàÎã§.
logging.basicConfig(
    level=logging.INFO,  # DEBUG Î†àÎ≤®Î°ú ÏÑ§Ï†ïÌïòÎ©¥ Îçî ÏûêÏÑ∏Ìïú Î°úÍ∑∏Î•º Î≥º Ïàò ÏûàÏäµÎãàÎã§.
    format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
    handlers=[
        logging.FileHandler("evaluation_log.txt", mode='a', encoding='utf-8'),
        logging.StreamHandler()  # ÏΩòÏÜîÏóêÎèÑ Î°úÍ∑∏ Ï∂úÎ†•
    ]
)

# --- Configuration ---
OLLAMA_MODEL_NAME = 'gemma3:12b'
INPUT_CSV_FILE = 'korean_llm_questions_updated.csv'
OUTPUT_CSV_FILE = 'llm_evaluation_results_xai.csv'
REQUEST_DELAY_SECONDS = 2

# --- Load Environment Variables ---
load_dotenv()
xai_api_key = os.getenv("XAI_API_KEY")
xai_model_name_env = os.getenv("XAI_MODEL_NAME")

if not xai_api_key:
    logging.error("‚ùå XAI_API_KEY not found in .env file.")
    exit()
else:
    key_display = xai_api_key[:7] + "..." + xai_api_key[-4:] if len(xai_api_key) > 11 else xai_api_key
    logging.info(f"‚ÑπÔ∏è XAI_API_KEY loaded (display: {key_display})")

if not xai_model_name_env:
    logging.warning("‚ö†Ô∏è XAI_MODEL_NAME not found in .env file. Using default 'grok-3-mini'.")
    XAI_MODEL_NAME = 'grok-3-mini'
else:
    XAI_MODEL_NAME = xai_model_name_env
    logging.info(f"‚ÑπÔ∏è Using xAI model: {XAI_MODEL_NAME} (from .env file)")

# --- Initialize xAI (via OpenAI compatible client) ---
try:
    xai_client = OpenAI(
        api_key=xai_api_key,
        base_url="https://api.x.ai/v1",
    )
    logging.info("‚úÖ xAI client initialized successfully.")
except Exception as e:
    logging.exception("‚ùå Error initializing xAI client.")
    exit()


def test_xai_api_key():
    logging.info(f"\nüß™ Testing xAI API key with model '{XAI_MODEL_NAME}'...")
    try:
        xai_client.chat.completions.create(
            model=XAI_MODEL_NAME,
            messages=[{"role": "user", "content": "Hello!"}],
            max_tokens=10,
            temperature=0.1
        )
        logging.info("‚úÖ xAI API key appears valid and model responded.")
        return True
    except AuthenticationError as e:
        logging.error(f"‚ùå xAI API Key is INVALID. Authentication failed. Details: {e}")
        return False
    except APIStatusError as e:
        logging.error(
            f"‚ùå Could not connect to xAI API or model issue (Status: {e.status_code}). Response: {e.response.text}")
        if e.status_code == 404:
            logging.error(f"   It seems the model '{XAI_MODEL_NAME}' was not found or you don't have access.")
        return False
    except Exception as e:
        logging.exception(f"‚ùå An unexpected error occurred while testing xAI API key.")
        return False


def get_ollama_response(prompt_text):
    try:
        logging.info(f"\n Ollama ({OLLAMA_MODEL_NAME}) <= Ï†ÑÏ≤¥ ÏßàÎ¨∏ÏÜ°Ïã† ÏöîÏ≤≠...")
        logging.debug(f"Ollama full prompt:\n{prompt_text}\n--------------------")
        response = ollama.chat(
            model=OLLAMA_MODEL_NAME,
            messages=[{'role': 'user', 'content': prompt_text}]
        )
        ollama_response_content = response['message']['content']
        logging.info(f" Ollama ({OLLAMA_MODEL_NAME}) => Ï†ÑÏ≤¥ ÎãµÎ≥ÄÏàòÏã† ÏôÑÎ£å.")
        logging.debug(f"Ollama full response:\n{ollama_response_content}\n--------------------")
        return ollama_response_content
    except Exception as e:
        logging.exception(f"‚ùå Error communicating with Ollama.")
        return "Error: Could not get response from Ollama."


def get_xai_evaluation(question, answer):
    evaluation_prompt = f"""
    You are an AI evaluator. Your task is to assess the appropriateness and correctness ("Ï†ïÌï©ÏÑ±")
    of a given answer to a specific question. Provide a percentage score from 0 to 100,
    where 0 is completely incorrect/irrelevant and 100 is perfectly correct and appropriate.
    Also, provide a brief justification for your score.

    Your response MUST strictly follow this format:
    Score: [Your numerical score between 0 and 100]
    Justification: [Your brief justification here]

    Original Question:
    {question}

    Answer to Evaluate:
    {answer}
    """
    try:
        logging.debug(
            f"\nDEBUG: xAI Grok ({XAI_MODEL_NAME}) <= Ï†ÑÏ≤¥ ÌèâÍ∞Ä ÌîÑÎ°¨ÌîÑÌä∏ Ï†ÑÎã¨ ÏòàÏ†ï:\n{evaluation_prompt}\n--------------------")
        logging.info(f" xAI Grok ({XAI_MODEL_NAME}) <= ÌèâÍ∞ÄÏöîÏ≤≠...")
        chat_completion = xai_client.chat.completions.create(
            model=XAI_MODEL_NAME,
            messages=[
                {"role": "user", "content": evaluation_prompt}
            ],
            temperature=0.2,
            max_tokens=2000,
        )

        logging.debug(f"DEBUG: Full xAI API Response Object:\n{chat_completion}\n--------------------")

        evaluation_text = ""
        if chat_completion.choices and len(chat_completion.choices) > 0:
            choice = chat_completion.choices[0]
            logging.debug(f"DEBUG: Finish Reason: {choice.finish_reason}")
            if choice.message:
                logging.debug(f"DEBUG: Message Object: {choice.message}")
                if choice.message.content is not None:
                    evaluation_text = choice.message.content
                    logging.debug(f"DEBUG: Message Content (evaluation_text):\n{evaluation_text}\n--------------------")
                else:
                    logging.debug("DEBUG: Message Content (evaluation_text) is None.")
            else:
                logging.debug("DEBUG: choice.message is None or empty.")
        else:
            logging.debug("DEBUG: chat_completion.choices is empty.")

        logging.info(f" xAI Grok ({XAI_MODEL_NAME}) => ÌèâÍ∞ÄÏàòÏã† (ÏùºÎ∂Ä): {evaluation_text[:100]}...")

        score_match = re.search(r"Score:\s*(\d+)", evaluation_text, re.IGNORECASE)
        justification_match = re.search(r"Justification:\s*(.+)", evaluation_text, re.IGNORECASE | re.DOTALL)

        score = int(score_match.group(1)) if score_match else -1
        justification = justification_match.group(
            1).strip() if justification_match else "Could not parse justification."

        if score == -1 and justification == "Could not parse justification.":
            justification = f"Failed to parse score/justification. Raw evaluation_text received: '{evaluation_text[:200]}'"

        return score, justification
    except AuthenticationError as e:
        logging.error(f"‚ùå xAI API Key became INVALID during evaluation. Details: {e}")
        return -1, f"xAI AuthenticationError: {e}"
    except APIStatusError as e:
        logging.error(
            f"‚ùå Error communicating with xAI API during evaluation (Status: {e.status_code}). Response: {e.response.text}")
        return -1, f"xAI APIStatusError (Status {e.status_code}): {e.message}"
    except Exception as e:
        logging.exception(f"‚ùå Unexpected error during xAI API call.")
        return -1, f"Unexpected error during xAI API call: {e}"


def main(start_from_question: int):  # ÏãúÏûë Î≤àÌò∏Î•º Ïù∏ÏûêÎ°ú Î∞õÏùå
    logging.info(
        f"üöÄ Starting LLM evaluation process (Targeting xAI API for evaluation). Starting from question number: {start_from_question}")

    if not test_xai_api_key():
        logging.error("\nExiting script due to xAI API key issue.")
        return

    logging.info("\nüìñ Loading dataset...")
    try:
        try:
            questions_df = pd.read_csv(INPUT_CSV_FILE, encoding='utf-8-sig')
        except UnicodeDecodeError:
            try:
                questions_df = pd.read_csv(INPUT_CSV_FILE, encoding='utf-8')
            except UnicodeDecodeError:
                questions_df = pd.read_csv(INPUT_CSV_FILE, encoding='cp949')
        logging.info(f"‚úÖ Successfully loaded {len(questions_df)} questions from '{INPUT_CSV_FILE}'.")

        prompt_column_name = 'Prompt_Korean'
        if prompt_column_name not in questions_df.columns:
            logging.error(f"‚ùå Column '{prompt_column_name}' not found in {INPUT_CSV_FILE}.")
            return
    except FileNotFoundError:
        logging.error(f"‚ùå Input CSV file '{INPUT_CSV_FILE}' not found.")
        return
    except UnicodeDecodeError as e:
        logging.error(f"‚ùå Could not decode CSV file. Tried 'utf-8-sig', 'utf-8', and 'cp949'. Final error: {e}")
        return
    except Exception as e:
        logging.exception(f"‚ùå Error reading CSV file.")
        return

    results = []
    total_score_sum = 0
    successfully_evaluated_count = 0

    start_index = start_from_question - 1  # 0-based indexÎ°ú Î≥ÄÌôò

    # CSV ÌååÏùº Ïù¥Ïñ¥Ïì∞Í∏∞ Î∞è Ìó§Îçî Ï≤òÎ¶¨ Î°úÏßÅ
    file_exists_and_has_content = False
    if os.path.exists(OUTPUT_CSV_FILE) and os.path.getsize(OUTPUT_CSV_FILE) > 0:
        file_exists_and_has_content = True

    # Ïû¨ÏãúÏûëÏù¥Í≥† ÌååÏùºÏóê ÎÇ¥Ïö©Ïù¥ ÏûàÏúºÎ©¥ Ïù¥Ïñ¥Ïì∞Í∏∞('a+'), ÏïÑÎãàÎ©¥ ÏÉàÎ°ú Ïì∞Í∏∞('w')
    open_mode = 'a+' if start_index > 0 and file_exists_and_has_content else 'w'

    logging.info(f"Output CSV '{OUTPUT_CSV_FILE}' will be opened in mode: '{open_mode}'.")

    try:
        with open(OUTPUT_CSV_FILE, open_mode, newline='', encoding='utf-8-sig') as csvfile:
            fieldnames = ['Question_ID', 'Category_Name_Korean', 'Category_Name_English',
                          'Prompt_Korean', 'Gemma3_Answer', 'XAI_Evaluation_Score', 'XAI_Evaluation_Justification']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            if open_mode == 'w' or not file_exists_and_has_content:
                logging.info("Writing CSV header.")
                writer.writeheader()
            else:
                logging.info("Appending to existing CSV. Header not written.")

            logging.info(f"\n‚öôÔ∏è Starting evaluation loop for {len(questions_df)} questions...")
            for index, row in questions_df.iterrows():
                current_question_number = index + 1

                if current_question_number < start_from_question:
                    logging.info(
                        f"Skipping already processed Question ID: {row.get('Question_ID', 'N/A')} (Number: {current_question_number})")
                    continue  # ÏßÄÏ†ïÎêú ÏãúÏûë Î≤àÌò∏ Ïù¥Ï†ÑÏùò ÏßàÎ¨∏ÏùÄ Í±¥ÎÑàÎúÄ

                logging.info(
                    f"\n--- ‚ùì Processing Question ID: {row.get('Question_ID', 'N/A')} (Number: {current_question_number}/{len(questions_df)}) ---")
                original_prompt = row[prompt_column_name]

                logging.info("Requesting response from Ollama...")
                gemma_answer = get_ollama_response(original_prompt)
                if "Error: Could not get response from Ollama." in gemma_answer:
                    logging.error("Ollama failed to provide an answer. Skipping evaluation for this question.")
                    xai_score, xai_justification = -1, "Ollama failed to respond."
                else:
                    logging.info("Response received from Ollama. Requesting evaluation from xAI...")
                    time.sleep(REQUEST_DELAY_SECONDS)
                    xai_score, xai_justification = get_xai_evaluation(original_prompt, gemma_answer)

                if xai_score != -1:
                    logging.info(f"  ‚û°Ô∏è  ÌèâÍ∞Ä Ï†êÏàò (xAI {XAI_MODEL_NAME}): {xai_score} / 100")
                    logging.info(f"  ‚û°Ô∏è  ÌèâÍ∞Ä Í∑ºÍ±∞: {xai_justification}")
                    total_score_sum += xai_score
                    successfully_evaluated_count += 1
                else:
                    logging.warning(f"  ‚û°Ô∏è  ÌèâÍ∞Ä Ïã§Ìå® ÎòêÎäî Ïò§Î•ò (xAI {XAI_MODEL_NAME})")
                    logging.warning(f"  ‚û°Ô∏è  Ïò§Î•ò/Ïã§Ìå® ÏÇ¨Ïú†: {xai_justification}")

                current_result = {
                    'Question_ID': row.get('Question_ID', ''),
                    'Category_Name_Korean': row.get('Category_Name_Korean', ''),
                    'Category_Name_English': row.get('Category_Name_English', ''),
                    'Prompt_Korean': original_prompt,
                    'Gemma3_Answer': gemma_answer,
                    'XAI_Evaluation_Score': xai_score,
                    'XAI_Evaluation_Justification': xai_justification
                }
                results.append(current_result)  # results Î¶¨Ïä§Ìä∏Îäî ÌòÑÏû¨ Ïã§ÌñâÏóêÏÑú Ï≤òÎ¶¨Îêú Í≤ÉÎßå Îã¥ÍπÄ
                writer.writerow(current_result)
                csvfile.flush()  # Í∞Å ÌñâÏùÑ Ïì∏ ÎïåÎßàÎã§ ÌååÏùºÏóê Ï¶âÏãú Î∞òÏòÅ
    except Exception as e:
        logging.exception("‚ùå An error occurred during the main evaluation loop or CSV writing.")

    logging.info(f"\n\nüéâ Evaluation complete for this run! üéâ")
    if successfully_evaluated_count > 0:
        average_score_this_run = total_score_sum / successfully_evaluated_count
        logging.info(f"üìä Ïù¥Î≤à Ïã§Ìñâ ÌèâÍ∞Ä ÏöîÏïΩ:")
        logging.info(f"  - Ïù¥Î≤à Ïã§ÌñâÏóêÏÑú ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÌèâÍ∞ÄÎêú ÏßàÎ¨∏ Ïàò: {successfully_evaluated_count}")
        logging.info(f"  - Ïù¥Î≤à Ïã§Ìñâ ÌèâÍ∑† Ï†êÏàò (0-100): {average_score_this_run:.2f}")
    else:
        logging.info("üìä Ïù¥Î≤à Ïã§Ìñâ ÌèâÍ∞Ä ÏöîÏïΩ: ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÌèâÍ∞ÄÎêú ÏßàÎ¨∏Ïù¥ ÏóÜÏäµÎãàÎã§.")
    logging.info(f"üíæ Results saved to/appended to '{OUTPUT_CSV_FILE}'")


if __name__ == "__main__":
    # --- Î™ÖÎ†πÏ§Ñ Ïù∏Ïûê ÌååÏã± ---
    parser = argparse.ArgumentParser(description="LLM Evaluation Script with Resume Functionality.")
    parser.add_argument(
        "--start_from",
        type=int,
        default=1,
        help="Question number (1-based) to start processing from. Defaults to 1 (start from the beginning)."
    )
    args = parser.parse_args()

    main(start_from_question=args.start_from)  # main Ìï®ÏàòÏóê ÏãúÏûë Î≤àÌò∏ Ï†ÑÎã¨