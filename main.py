import ollama
import pandas as pd
from openai import OpenAI, AuthenticationError, APIStatusError
import os
from dotenv import load_dotenv
import re
import time
import csv
import logging  # ë¡œê¹… ëª¨ë“ˆ ì¶”ê°€
import argparse  # ì¸ì íŒŒì‹± ëª¨ë“ˆ ì¶”ê°€

# --- ë¡œê¹… ì„¤ì • ---
# ë¡œê·¸ íŒŒì¼ëª…, ë¡œê·¸ ë ˆë²¨, ë¡œê·¸ í¬ë§· ë“±ì„ ì„¤ì •í•©ë‹ˆë‹¤.
# ë¡œê·¸ íŒŒì¼ì€ 'evaluation_log.txt'ì— ì €ì¥ë˜ë©°, ì‹¤í–‰ ì‹œë§ˆë‹¤ ë‚´ìš©ì´ ì¶”ê°€ë©ë‹ˆë‹¤.
logging.basicConfig(
    level=logging.INFO,  # DEBUG ë ˆë²¨ë¡œ ì„¤ì •í•˜ë©´ ë” ìì„¸í•œ ë¡œê·¸ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
    handlers=[
        logging.FileHandler("evaluation_log.txt", mode='a', encoding='utf-8'),
        logging.StreamHandler()  # ì½˜ì†”ì—ë„ ë¡œê·¸ ì¶œë ¥
    ]
)

# --- Configuration ---
OLLAMA_MODEL_NAME = 'gemma3:12b'
INPUT_CSV_FILE = 'korean_llm_questions_updated.csv'
OUTPUT_CSV_FILE = 'llm_evaluation_results_xai.csv'
REQUEST_DELAY_SECONDS = 2

# --- Load Environment Variables ---
load_dotenv()
xai_api_key = os.getenv("XAI_API_KEY")
xai_model_name_env = os.getenv("XAI_MODEL_NAME")

if not xai_api_key:
    logging.error("âŒ XAI_API_KEY not found in .env file.")
    exit()
else:
    key_display = xai_api_key[:7] + "..." + xai_api_key[-4:] if len(xai_api_key) > 11 else xai_api_key
    logging.info(f"â„¹ï¸ XAI_API_KEY loaded (display: {key_display})")

if not xai_model_name_env:
    logging.warning("âš ï¸ XAI_MODEL_NAME not found in .env file. Using default 'grok-3-mini'.")
    XAI_MODEL_NAME = 'grok-3-mini'
else:
    XAI_MODEL_NAME = xai_model_name_env
    logging.info(f"â„¹ï¸ Using xAI model: {XAI_MODEL_NAME} (from .env file)")

# --- Initialize xAI (via OpenAI compatible client) ---
try:
    xai_client = OpenAI(
        api_key=xai_api_key,
        base_url="https://api.x.ai/v1",
    )
    logging.info("âœ… xAI client initialized successfully.")
except Exception as e:
    logging.exception("âŒ Error initializing xAI client.")
    exit()


def test_xai_api_key():
    logging.info(f"\nğŸ§ª Testing xAI API key with model '{XAI_MODEL_NAME}'...")
    try:
        xai_client.chat.completions.create(
            model=XAI_MODEL_NAME,
            messages=[{"role": "user", "content": "Hello!"}],
            max_tokens=10,
            temperature=0.1
        )
        logging.info("âœ… xAI API key appears valid and model responded.")
        return True
    except AuthenticationError as e:
        logging.error(f"âŒ xAI API Key is INVALID. Authentication failed. Details: {e}")
        return False
    except APIStatusError as e:
        logging.error(
            f"âŒ Could not connect to xAI API or model issue (Status: {e.status_code}). Response: {e.response.text}")
        if e.status_code == 404:
            logging.error(f"   It seems the model '{XAI_MODEL_NAME}' was not found or you don't have access.")
        return False
    except Exception as e:
        logging.exception(f"âŒ An unexpected error occurred while testing xAI API key.")
        return False


def get_ollama_response(prompt_text):
    try:
        logging.info(f"\n Ollama ({OLLAMA_MODEL_NAME}) <= ì „ì²´ ì§ˆë¬¸ì†¡ì‹  ìš”ì²­...")
        logging.debug(f"Ollama full prompt:\n{prompt_text}\n--------------------")
        response = ollama.chat(
            model=OLLAMA_MODEL_NAME,
            messages=[{'role': 'user', 'content': prompt_text}]
        )
        ollama_response_content = response['message']['content']
        logging.info(f" Ollama ({OLLAMA_MODEL_NAME}) => ì „ì²´ ë‹µë³€ìˆ˜ì‹  ì™„ë£Œ.")
        logging.debug(f"Ollama full response:\n{ollama_response_content}\n--------------------")
        return ollama_response_content
    except Exception as e:
        logging.exception(f"âŒ Error communicating with Ollama.")
        return "Error: Could not get response from Ollama."


def get_xai_evaluation(question, answer):
    evaluation_prompt = f"""
    You are an AI evaluator. Your task is to assess the appropriateness and correctness ("ì •í•©ì„±")
    of a given answer to a specific question. Provide a percentage score from 0 to 100,
    where 0 is completely incorrect/irrelevant and 100 is perfectly correct and appropriate.
    Also, provide a brief justification for your score.

    Your response MUST strictly follow this format:
    Score: [Your numerical score between 0 and 100]
    Justification: [Your brief justification here]

    Original Question:
    {question}

    Answer to Evaluate:
    {answer}
    """
    try:
        logging.debug(
            f"\nDEBUG: xAI Grok ({XAI_MODEL_NAME}) <= ì „ì²´ í‰ê°€ í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ì˜ˆì •:\n{evaluation_prompt}\n--------------------")
        logging.info(f" xAI Grok ({XAI_MODEL_NAME}) <= í‰ê°€ìš”ì²­...")
        chat_completion = xai_client.chat.completions.create(
            model=XAI_MODEL_NAME,
            messages=[
                {"role": "user", "content": evaluation_prompt}
            ],
            temperature=0.2,
            max_tokens=2000,
        )

        logging.debug(f"DEBUG: Full xAI API Response Object:\n{chat_completion}\n--------------------")

        evaluation_text = ""
        if chat_completion.choices and len(chat_completion.choices) > 0:
            choice = chat_completion.choices[0]
            logging.debug(f"DEBUG: Finish Reason: {choice.finish_reason}")
            if choice.message:
                logging.debug(f"DEBUG: Message Object: {choice.message}")
                if choice.message.content is not None:
                    evaluation_text = choice.message.content
                    logging.debug(f"DEBUG: Message Content (evaluation_text):\n{evaluation_text}\n--------------------")
                else:
                    logging.debug("DEBUG: Message Content (evaluation_text) is None.")
            else:
                logging.debug("DEBUG: choice.message is None or empty.")
        else:
            logging.debug("DEBUG: chat_completion.choices is empty.")

        logging.info(f" xAI Grok ({XAI_MODEL_NAME}) => í‰ê°€ìˆ˜ì‹  (ì¼ë¶€): {evaluation_text[:100]}...")

        score_match = re.search(r"Score:\s*(\d+)", evaluation_text, re.IGNORECASE)
        justification_match = re.search(r"Justification:\s*(.+)", evaluation_text, re.IGNORECASE | re.DOTALL)

        score = int(score_match.group(1)) if score_match else -1
        justification = justification_match.group(
            1).strip() if justification_match else "Could not parse justification."

        if score == -1 and justification == "Could not parse justification.":
            justification = f"Failed to parse score/justification. Raw evaluation_text received: '{evaluation_text[:200]}'"

        return score, justification
    except AuthenticationError as e:
        logging.error(f"âŒ xAI API Key became INVALID during evaluation. Details: {e}")
        return -1, f"xAI AuthenticationError: {e}"
    except APIStatusError as e:
        logging.error(
            f"âŒ Error communicating with xAI API during evaluation (Status: {e.status_code}). Response: {e.response.text}")
        return -1, f"xAI APIStatusError (Status {e.status_code}): {e.message}"
    except Exception as e:
        logging.exception(f"âŒ Unexpected error during xAI API call.")
        return -1, f"Unexpected error during xAI API call: {e}"


def main(start_from_question: int):  # ì‹œì‘ ë²ˆí˜¸ë¥¼ ì¸ìë¡œ ë°›ìŒ
    logging.info(
        f"ğŸš€ Starting LLM evaluation process (Targeting xAI API for evaluation). Starting from question number: {start_from_question}")

    if not test_xai_api_key():
        logging.error("\nExiting script due to xAI API key issue.")
        return

    logging.info("\nğŸ“– Loading dataset...")
    try:
        try:
            questions_df = pd.read_csv(INPUT_CSV_FILE, encoding='utf-8-sig')
        except UnicodeDecodeError:
            try:
                questions_df = pd.read_csv(INPUT_CSV_FILE, encoding='utf-8')
            except UnicodeDecodeError:
                questions_df = pd.read_csv(INPUT_CSV_FILE, encoding='cp949')
        logging.info(f"âœ… Successfully loaded {len(questions_df)} questions from '{INPUT_CSV_FILE}'.")

        prompt_column_name = 'Prompt_Korean'
        if prompt_column_name not in questions_df.columns:
            logging.error(f"âŒ Column '{prompt_column_name}' not found in {INPUT_CSV_FILE}.")
            return
    except FileNotFoundError:
        logging.error(f"âŒ Input CSV file '{INPUT_CSV_FILE}' not found.")
        return
    except UnicodeDecodeError as e:
        logging.error(f"âŒ Could not decode CSV file. Tried 'utf-8-sig', 'utf-8', and 'cp949'. Final error: {e}")
        return
    except Exception as e:
        logging.exception(f"âŒ Error reading CSV file.")
        return

    results = []
    total_score_sum = 0
    successfully_evaluated_count = 0

    start_index = start_from_question - 1  # 0-based indexë¡œ ë³€í™˜

    # CSV íŒŒì¼ ì´ì–´ì“°ê¸° ë° í—¤ë” ì²˜ë¦¬ ë¡œì§
    file_exists_and_has_content = False
    if os.path.exists(OUTPUT_CSV_FILE) and os.path.getsize(OUTPUT_CSV_FILE) > 0:
        file_exists_and_has_content = True

    # ì¬ì‹œì‘ì´ê³  íŒŒì¼ì— ë‚´ìš©ì´ ìˆìœ¼ë©´ ì´ì–´ì“°ê¸°('a+'), ì•„ë‹ˆë©´ ìƒˆë¡œ ì“°ê¸°('w')
    open_mode = 'a+' if start_index > 0 and file_exists_and_has_content else 'w'

    logging.info(f"Output CSV '{OUTPUT_CSV_FILE}' will be opened in mode: '{open_mode}'.")

    try:
        with open(OUTPUT_CSV_FILE, open_mode, newline='', encoding='utf-8-sig') as csvfile:
            fieldnames = ['Question_ID', 'Category_Name_Korean', 'Category_Name_English',
                          'Prompt_Korean', 'Gemma3_Answer', 'XAI_Evaluation_Score', 'XAI_Evaluation_Justification']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            if open_mode == 'w' or not file_exists_and_has_content:
                logging.info("Writing CSV header.")
                writer.writeheader()
            else:
                logging.info("Appending to existing CSV. Header not written.")

            logging.info(f"\nâš™ï¸ Starting evaluation loop for {len(questions_df)} questions...")
            for index, row in questions_df.iterrows():
                current_question_number = index + 1

                if current_question_number < start_from_question:
                    logging.info(
                        f"Skipping already processed Question ID: {row.get('Question_ID', 'N/A')} (Number: {current_question_number})")
                    continue  # ì§€ì •ëœ ì‹œì‘ ë²ˆí˜¸ ì´ì „ì˜ ì§ˆë¬¸ì€ ê±´ë„ˆëœ€

                logging.info(
                    f"\n--- â“ Processing Question ID: {row.get('Question_ID', 'N/A')} (Number: {current_question_number}/{len(questions_df)}) ---")
                original_prompt = row[prompt_column_name]

                logging.info("Requesting response from Ollama...")
                gemma_answer = get_ollama_response(original_prompt)
                if "Error: Could not get response from Ollama." in gemma_answer:
                    logging.error("Ollama failed to provide an answer. Skipping evaluation for this question.")
                    xai_score, xai_justification = -1, "Ollama failed to respond."
                else:
                    logging.info("Response received from Ollama. Requesting evaluation from xAI...")
                    time.sleep(REQUEST_DELAY_SECONDS)
                    xai_score, xai_justification = get_xai_evaluation(original_prompt, gemma_answer)

                if xai_score != -1:
                    logging.info(f"  â¡ï¸  í‰ê°€ ì ìˆ˜ (xAI {XAI_MODEL_NAME}): {xai_score} / 100")
                    logging.info(f"  â¡ï¸  í‰ê°€ ê·¼ê±°: {xai_justification}")
                    total_score_sum += xai_score
                    successfully_evaluated_count += 1
                else:
                    logging.warning(f"  â¡ï¸  í‰ê°€ ì‹¤íŒ¨ ë˜ëŠ” ì˜¤ë¥˜ (xAI {XAI_MODEL_NAME})")
                    logging.warning(f"  â¡ï¸  ì˜¤ë¥˜/ì‹¤íŒ¨ ì‚¬ìœ : {xai_justification}")

                current_result = {
                    'Question_ID': row.get('Question_ID', ''),
                    'Category_Name_Korean': row.get('Category_Name_Korean', ''),
                    'Category_Name_English': row.get('Category_Name_English', ''),
                    'Prompt_Korean': original_prompt,
                    'Gemma3_Answer': gemma_answer,
                    'XAI_Evaluation_Score': xai_score,
                    'XAI_Evaluation_Justification': xai_justification
                }
                results.append(current_result)  # results ë¦¬ìŠ¤íŠ¸ëŠ” í˜„ì¬ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬ëœ ê²ƒë§Œ ë‹´ê¹€
                writer.writerow(current_result)
                csvfile.flush()  # ê° í–‰ì„ ì“¸ ë•Œë§ˆë‹¤ íŒŒì¼ì— ì¦‰ì‹œ ë°˜ì˜
    except Exception as e:
        logging.exception("âŒ An error occurred during the main evaluation loop or CSV writing.")

    logging.info(f"\n\nğŸ‰ Evaluation complete for this run! ğŸ‰")
    if successfully_evaluated_count > 0:
        average_score_this_run = total_score_sum / successfully_evaluated_count
        logging.info(f"ğŸ“Š ì´ë²ˆ ì‹¤í–‰ í‰ê°€ ìš”ì•½:")
        logging.info(f"  - ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì„±ê³µì ìœ¼ë¡œ í‰ê°€ëœ ì§ˆë¬¸ ìˆ˜: {successfully_evaluated_count}")
        logging.info(f"  - ì´ë²ˆ ì‹¤í–‰ í‰ê·  ì ìˆ˜ (0-100): {average_score_this_run:.2f}")
    else:
        logging.info("ğŸ“Š ì´ë²ˆ ì‹¤í–‰ í‰ê°€ ìš”ì•½: ì„±ê³µì ìœ¼ë¡œ í‰ê°€ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    logging.info(f"ğŸ’¾ Results saved to/appended to '{OUTPUT_CSV_FILE}'")


if __name__ == "__main__":
    # --- ëª…ë ¹ì¤„ ì¸ì íŒŒì‹± ---
    parser = argparse.ArgumentParser(description="LLM Evaluation Script with Resume Functionality.")
    parser.add_argument(
        "--start_from",
        type=int,
        default=1,
        help="Question number (1-based) to start processing from. Defaults to 1 (start from the beginning)."
    )
    args = parser.parse_args()

    main(start_from_question=args.start_from)  # main í•¨ìˆ˜ì— ì‹œì‘ ë²ˆí˜¸ ì „ë‹¬